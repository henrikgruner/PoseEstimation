Traceback (most recent call last):
  File "main.py", line 333, in <module>
    train_loss, train_angle_errors, geodesic_train = train(
  File "main.py", line 185, in train
    model_out = model(gg)
  File "/cluster/home/henrgr/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/cluster/home/henrgr/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py", line 168, in forward
    outputs = self.parallel_apply(replicas, inputs, kwargs)
  File "/cluster/home/henrgr/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py", line 178, in parallel_apply
    return parallel_apply(replicas, inputs, kwargs, self.device_ids[:len(replicas)])
  File "/cluster/home/henrgr/.local/lib/python3.8/site-packages/torch/nn/parallel/parallel_apply.py", line 86, in parallel_apply
    output.reraise()
  File "/cluster/home/henrgr/.local/lib/python3.8/site-packages/torch/_utils.py", line 434, in reraise
    raise exception
RuntimeError: Caught RuntimeError in replica 0 on device 0.
Original Traceback (most recent call last):
  File "/cluster/home/henrgr/.local/lib/python3.8/site-packages/torch/nn/parallel/parallel_apply.py", line 61, in _worker
    output = module(*input, **kwargs)
  File "/cluster/home/henrgr/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/cluster/home/henrgr/so3master/point_cloud/model_fetch.py", line 169, in forward
    out_nd = self.model(input)
  File "/cluster/home/henrgr/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/cluster/home/henrgr/so3master/point_cloud/pointnet.py", line 85, in forward
    l2_xyz, l2_points = self.sa2(l1_xyz, l1_points)
  File "/cluster/home/henrgr/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/cluster/home/henrgr/so3master/point_cloud/pointnet_utils.py", line 246, in forward
    grouped_points = F.relu(bn(conv(grouped_points)), inplace=True)
  File "/cluster/home/henrgr/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/cluster/home/henrgr/.local/lib/python3.8/site-packages/torch/nn/modules/conv.py", line 446, in forward
    return self._conv_forward(input, self.weight, self.bias)
  File "/cluster/home/henrgr/.local/lib/python3.8/site-packages/torch/nn/modules/conv.py", line 442, in _conv_forward
    return F.conv2d(input, weight, bias, self.stride,
RuntimeError: CUDA out of memory. Tried to allocate 1.34 GiB (GPU 0; 31.75 GiB total capacity; 28.00 GiB already allocated; 327.69 MiB free; 29.68 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

ERROR: Could not open requirements file: [Errno 2] No such file or directory: 'requirements.txt'
chair
dataset size: 889
cuda:  True
count:  4
Maximum batch size in power of two is : 64
names:  ['Tesla V100-PCIE-32GB', 'Tesla V100-PCIE-32GB', 'Tesla V100-PCIE-32GB', 'Tesla V100-PCIE-32GB']
device ids: [0, 1, 2, 3]
0 / 250 :  Training loss:  2.1 train-MAE:  107.7 Median-Geo:  1.489 geo-train 1.534 Validation loss:  2.2 Val-MAE:  111.6 Median-geo:  1.491 geo 1.585 time:  41.838054180145264
1 / 250 :  Training loss:  1.8 train-MAE:  90.0 Median-Geo:  1.626 geo-train 1.566 Validation loss:  2.0 Val-MAE:  99.8 Median-geo:  1.417 geo 1.398 time:  30.912121295928955
CUDA runtime error: an illegal memory access was encountered (700) in apply_lu_batched_magma at ../aten/src/ATen/native/cuda/BatchLinearAlgebra.cpp:1920
CUDA runtime error: an illegal memory access was encountered (700) in magma_queue_destroy_internal at /opt/conda/conda-bld/magma-cuda113_1619629459349/work/interface_cuda/interface.cpp:944
CUDA runtime error: an illegal memory access was encountered (700) in magma_queue_destroy_internal at /opt/conda/conda-bld/magma-cuda113_1619629459349/work/interface_cuda/interface.cpp:945
CUDA runtime error: an illegal memory access was encountered (700) in magma_queue_destroy_internal at /opt/conda/conda-bld/magma-cuda113_1619629459349/work/interface_cuda/interface.cpp:946
CUDA runtime error: an illegal memory access was encountered (700) in magma_queue_destroy_internal at /opt/conda/conda-bld/magma-cuda113_1619629459349/work/interface_cuda/interface.cpp:945
CUDA runtime error: an illegal memory access was encountered (700) in magma_queue_destroy_internal at /opt/conda/conda-bld/magma-cuda113_1619629459349/work/interface_cuda/interface.cpp:946
CUDA runtime error: an illegal memory access was encountered (700) in magma_queue_destroy_internal at /opt/conda/conda-bld/magma-cuda113_1619629459349/work/interface_cuda/interface.cpp:946
Traceback (most recent call last):
  File "main.py", line 333, in <module>
    train_loss, train_angle_errors, geodesic_train = train(
  File "main.py", line 185, in train
    model_out = model(gg)
  File "/cluster/home/henrgr/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/cluster/home/henrgr/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py", line 168, in forward
    outputs = self.parallel_apply(replicas, inputs, kwargs)
  File "/cluster/home/henrgr/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py", line 178, in parallel_apply
    return parallel_apply(replicas, inputs, kwargs, self.device_ids[:len(replicas)])
  File "/cluster/home/henrgr/.local/lib/python3.8/site-packages/torch/nn/parallel/parallel_apply.py", line 86, in parallel_apply
    output.reraise()
  File "/cluster/home/henrgr/.local/lib/python3.8/site-packages/torch/_utils.py", line 434, in reraise
    raise exception
RuntimeError: Caught RuntimeError in replica 0 on device 0.
Original Traceback (most recent call last):
  File "/cluster/home/henrgr/.local/lib/python3.8/site-packages/torch/nn/parallel/parallel_apply.py", line 61, in _worker
    output = module(*input, **kwargs)
  File "/cluster/home/henrgr/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/cluster/home/henrgr/so3master/point_cloud/model_fetch.py", line 173, in forward
    out_rmat = symmetric_orthogonalization(out_nd)
  File "/cluster/home/henrgr/so3master/point_cloud/model_fetch.py", line 21, in symmetric_orthogonalization
    det = torch.det(torch.bmm(u, vt))
RuntimeError: CUDA error: an illegal memory access was encountered
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.

bed
dataset size: 515
cuda:  True
count:  4
Maximum batch size in power of two is : 64
names:  ['Tesla V100-PCIE-32GB', 'Tesla V100-PCIE-32GB', 'Tesla V100-PCIE-32GB', 'Tesla V100-PCIE-32GB']
device ids: [0, 1, 2, 3]
0 / 250 :  Training loss:  2.3 train-MAE:  123.6 Median-Geo:  2.088 geo-train 1.977 Validation loss:  2.3 Val-MAE:  115.0 Median-geo:  1.899 geo 2.009 time:  41.19747972488403
1 / 250 :  Training loss:  1.9 train-MAE:  93.3 Median-Geo:  1.514 geo-train 1.468 Validation loss:  1.9 Val-MAE:  97.5 Median-geo:  1.033 geo 1.359 time:  30.41517424583435
2 / 250 :  Training loss:  1.7 train-MAE:  81.3 Median-Geo:  0.803 geo-train 1.042 Validation loss:  1.9 Val-MAE:  96.5 Median-geo:  1.089 geo 1.319 time:  30.572912216186523
3 / 250 :  Training loss:  1.9 train-MAE:  95.4 Median-Geo:  1.536 geo-train 1.751 Validation loss:  1.9 Val-MAE:  95.3 Median-geo:  0.82 geo 1.111 time:  30.565985918045044
4 / 250 :  Training loss:  2.0 train-MAE:  100.0 Median-Geo:  2.097 geo-train 2.055 Validation loss:  1.9 Val-MAE:  95.4 Median-geo:  0.929 geo 1.213 time:  30.635806798934937
5 / 250 :  Training loss:  1.7 train-MAE:  81.2 Median-Geo:  1.102 geo-train 1.036 Validation loss:  1.9 Val-MAE:  97.6 Median-geo:  1.033 geo 1.25 time:  31.05454993247986
6 / 250 :  Training loss:  1.9 train-MAE:  97.5 Median-Geo:  2.226 geo-train 2.029 Validation loss:  1.9 Val-MAE:  95.8 Median-geo:  0.97 geo 1.268 time:  31.607751846313477
7 / 250 :  Training loss:  2.1 train-MAE:  106.9 Median-Geo:  2.642 geo-train 2.493 Validation loss:  1.9 Val-MAE:  94.3 Median-geo:  0.913 geo 1.18 time:  31.875438928604126
8 / 250 :  Training loss:  1.9 train-MAE:  100.5 Median-Geo:  2.568 geo-train 2.083 Validation loss:  1.9 Val-MAE:  96.4 Median-geo:  0.903 geo 1.304 time:  31.739424228668213
9 / 250 :  Training loss:  1.9 train-MAE:  98.4 Median-Geo:  2.424 geo-train 2.095 Validation loss:  1.9 Val-MAE:  94.9 Median-geo:  0.927 geo 1.174 time:  31.94739079475403
10 / 250 :  Training loss:  2.0 train-MAE:  105.6 Median-Geo:  2.757 geo-train 2.499 Validation loss:  1.9 Val-MAE:  94.8 Median-geo:  0.914 geo 1.168 time:  32.232537508010864
11 / 250 :  Training loss:  1.9 train-MAE:  94.5 Median-Geo:  2.032 geo-train 1.949 Validation loss:  1.9 Val-MAE:  95.8 Median-geo:  0.864 geo 1.243 time:  30.506637811660767
12 / 250 :  Training loss:  1.6 train-MAE:  78.1 Median-Geo:  0.705 geo-train 1.006 Validation loss:  1.9 Val-MAE:  97.3 Median-geo:  0.905 geo 1.324 time:  30.599475145339966
13 / 250 :  Training loss:  2.0 train-MAE:  96.0 Median-Geo:  1.926 geo-train 2.009 Validation loss:  1.8 Val-MAE:  92.9 Median-geo:  0.82 geo 1.089 time:  30.393261671066284
14 / 250 :  Training loss:  1.6 train-MAE:  76.1 Median-Geo:  1.37 geo-train 1.07 Validation loss:  1.9 Val-MAE:  96.2 Median-geo:  1.022 geo 1.255 time:  30.374389171600342
15 / 250 :  Training loss:  1.9 train-MAE:  92.7 Median-Geo:  1.577 geo-train 1.584 Validation loss:  1.8 Val-MAE:  93.1 Median-geo:  0.746 geo 1.053 time:  30.539875507354736
16 / 250 :  Training loss:  1.4 train-MAE:  69.2 Median-Geo:  0.575 geo-train 0.606 Validation loss:  1.9 Val-MAE:  96.4 Median-geo:  0.867 geo 1.237 time:  30.558257818222046
17 / 250 :  Training loss:  1.5 train-MAE:  71.4 Median-Geo:  0.474 geo-train 0.647 Validation loss:  1.8 Val-MAE:  92.3 Median-geo:  0.617 geo 0.975 time:  30.914846181869507
18 / 250 :  Training loss:  1.7 train-MAE:  84.4 Median-Geo:  1.341 geo-train 1.286 Validation loss:  1.8 Val-MAE:  92.4 Median-geo:  0.77 geo 1.062 time:  30.401784658432007
19 / 250 :  Training loss:  1.8 train-MAE:  89.2 Median-Geo:  1.508 geo-train 1.497 Validation loss:  1.8 Val-MAE:  92.8 Median-geo:  0.534 geo 0.955 time:  30.38417911529541
20 / 250 :  Training loss:  1.7 train-MAE:  84.3 Median-Geo:  1.142 geo-train 1.292 Validation loss:  1.8 Val-MAE:  92.9 Median-geo:  0.581 geo 0.958 time:  30.472020387649536
21 / 250 :  Training loss:  1.7 train-MAE:  81.6 Median-Geo:  1.479 geo-train 1.258 Validation loss:  1.8 Val-MAE:  91.6 Median-geo:  0.535 geo 0.935 time:  30.571661233901978
22 / 250 :  Training loss:  1.9 train-MAE:  96.3 Median-Geo:  2.427 geo-train 1.936 Validation loss:  1.8 Val-MAE:  90.2 Median-geo:  0.428 geo 0.891 time:  30.560003757476807
23 / 250 :  Training loss:  1.9 train-MAE:  100.7 Median-Geo:  2.514 geo-train 2.168 Validation loss:  1.8 Val-MAE:  89.7 Median-geo:  0.418 geo 0.932 time:  30.473622798919678
24 / 250 :  Training loss:  1.9 train-MAE:  100.3 Median-Geo:  2.299 geo-train 2.19 Validation loss:  1.8 Val-MAE:  89.4 Median-geo:  0.482 geo 0.912 time:  30.729419469833374
25 / 250 :  Training loss:  1.8 train-MAE:  90.6 Median-Geo:  1.516 geo-train 1.708 Validation loss:  1.7 Val-MAE:  88.2 Median-geo:  0.351 geo 0.866 time:  30.553691148757935
26 / 250 :  Training loss:  1.5 train-MAE:  71.1 Median-Geo:  1.157 geo-train 0.837 Validation loss:  1.8 Val-MAE:  90.8 Median-geo:  0.504 geo 0.949 time:  31.910483598709106
27 / 250 :  Training loss:  2.0 train-MAE:  100.9 Median-Geo:  2.595 geo-train 2.221 Validation loss:  1.7 Val-MAE:  89.0 Median-geo:  0.373 geo 0.877 time:  30.75435471534729
28 / 250 :  Training loss:  1.5 train-MAE:  74.2 Median-Geo:  1.099 geo-train 0.838 Validation loss:  1.7 Val-MAE:  86.7 Median-geo:  0.256 geo 0.799 time:  31.037122011184692
29 / 250 :  Training loss:  1.4 train-MAE:  67.6 Median-Geo:  0.236 geo-train 0.609 Validation loss:  1.8 Val-MAE:  92.4 Median-geo:  0.615 geo 1.128 time:  31.073102712631226
30 / 250 :  Training loss:  1.9 train-MAE:  97.5 Median-Geo:  2.608 geo-train 2.064 Validation loss:  1.7 Val-MAE:  88.7 Median-geo:  0.395 geo 0.938 time:  31.377814769744873
31 / 250 :  Training loss:  1.6 train-MAE:  81.2 Median-Geo:  0.571 geo-train 1.41 Validation loss:  1.7 Val-MAE:  86.8 Median-geo:  0.301 geo 0.849 time:  30.721776723861694
32 / 250 :  Training loss:  1.7 train-MAE:  86.5 Median-Geo:  1.247 geo-train 1.55 Validation loss:  1.8 Val-MAE:  90.0 Median-geo:  0.519 geo 0.937 time:  30.767508506774902
33 / 250 :  Training loss:  1.7 train-MAE:  84.1 Median-Geo:  1.375 geo-train 1.64 Validation loss:  1.7 Val-MAE:  87.8 Median-geo:  0.401 geo 0.834 time:  30.453234672546387
Traceback (most recent call last):
  File "main.py", line 333, in <module>
    train_loss, train_angle_errors, geodesic_train = train(
  File "main.py", line 187, in train
    angle = angle_error(model_out, gt_rmat).mean().item()
  File "main.py", line 151, in angle_error
    raise ValueError(
ValueError: angle out of range, input probably not proper rotation matrices
sofa
dataset size: 680
cuda:  True
count:  4
Maximum batch size in power of two is : 64
names:  ['Tesla V100-PCIE-32GB', 'Tesla V100-PCIE-32GB', 'Tesla V100-PCIE-32GB', 'Tesla V100-PCIE-32GB']
device ids: [0, 1, 2, 3]
0 / 250 :  Training loss:  2.4 train-MAE:  130.2 Median-Geo:  1.842 geo-train 1.907 Validation loss:  1.9 Val-MAE:  96.7 Median-geo:  2.192 geo 1.88 time:  40.0632061958313
1 / 250 :  Training loss:  1.9 train-MAE:  95.5 Median-Geo:  2.192 geo-train 1.901 Validation loss:  1.9 Val-MAE:  93.9 Median-geo:  1.754 geo 1.676 time:  30.69650411605835
2 / 250 :  Training loss:  1.7 train-MAE:  85.5 Median-Geo:  1.946 geo-train 1.75 Validation loss:  1.9 Val-MAE:  96.0 Median-geo:  1.864 geo 1.681 time:  30.46472668647766
3 / 250 :  Training loss:  1.7 train-MAE:  87.2 Median-Geo:  1.276 geo-train 1.407 Validation loss:  1.9 Val-MAE:  96.0 Median-geo:  1.975 geo 1.767 time:  29.119046926498413
4 / 250 :  Training loss:  1.6 train-MAE:  84.4 Median-Geo:  0.825 geo-train 1.377 Validation loss:  1.9 Val-MAE:  97.5 Median-geo:  1.832 geo 1.7 time:  29.289180278778076
5 / 250 :  Training loss:  1.7 train-MAE:  85.6 Median-Geo:  1.675 geo-train 1.687 Validation loss:  1.9 Val-MAE:  96.1 Median-geo:  2.144 geo 1.863 time:  29.319374799728394
CUDA runtime error: an illegal memory access was encountered (700) in apply_lu_batched_magma at ../aten/src/ATen/native/cuda/BatchLinearAlgebra.cpp:1920
CUDA runtime error: an illegal memory access was encountered (700) in magma_queue_destroy_internal at /opt/conda/conda-bld/magma-cuda113_1619629459349/work/interface_cuda/interface.cpp:944
CUDA runtime error: an illegal memory access was encountered (700) in magma_queue_destroy_internal at /opt/conda/conda-bld/magma-cuda113_1619629459349/work/interface_cuda/interface.cpp:945
CUDA runtime error: an illegal memory access was encountered (700) in magma_queue_destroy_internal at /opt/conda/conda-bld/magma-cuda113_1619629459349/work/interface_cuda/interface.cpp:946
CUDA runtime error: an illegal memory access was encountered (700) in magma_queue_destroy_internal at /opt/conda/conda-bld/magma-cuda113_1619629459349/work/interface_cuda/interface.cpp:945
CUDA runtime error: an illegal memory access was encountered (700) in magma_queue_destroy_internal at /opt/conda/conda-bld/magma-cuda113_1619629459349/work/interface_cuda/interface.cpp:946
Traceback (most recent call last):
  File "main.py", line 333, in <module>
    train_loss, train_angle_errors, geodesic_train = train(
  File "main.py", line 185, in train
    model_out = model(gg)
  File "/cluster/home/henrgr/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/cluster/home/henrgr/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py", line 168, in forward
    outputs = self.parallel_apply(replicas, inputs, kwargs)
  File "/cluster/home/henrgr/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py", line 178, in parallel_apply
    return parallel_apply(replicas, inputs, kwargs, self.device_ids[:len(replicas)])
  File "/cluster/home/henrgr/.local/lib/python3.8/site-packages/torch/nn/parallel/parallel_apply.py", line 86, in parallel_apply
    output.reraise()
  File "/cluster/home/henrgr/.local/lib/python3.8/site-packages/torch/_utils.py", line 434, in reraise
    raise exception
RuntimeError: Caught RuntimeError in replica 0 on device 0.
Original Traceback (most recent call last):
  File "/cluster/home/henrgr/.local/lib/python3.8/site-packages/torch/nn/parallel/parallel_apply.py", line 61, in _worker
    output = module(*input, **kwargs)
  File "/cluster/home/henrgr/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/cluster/home/henrgr/so3master/point_cloud/model_fetch.py", line 173, in forward
    out_rmat = symmetric_orthogonalization(out_nd)
  File "/cluster/home/henrgr/so3master/point_cloud/model_fetch.py", line 21, in symmetric_orthogonalization
    det = torch.det(torch.bmm(u, vt))
RuntimeError: CUDA error: an illegal memory access was encountered
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.

toilet
dataset size: 344
cuda:  True
count:  4
Maximum batch size in power of two is : 64
names:  ['Tesla V100-PCIE-32GB', 'Tesla V100-PCIE-32GB', 'Tesla V100-PCIE-32GB', 'Tesla V100-PCIE-32GB']
device ids: [0, 1, 2, 3]
0 / 250 :  Training loss:  2.5 train-MAE:  135.6 Median-Geo:  2.533 geo-train 2.453 Validation loss:  2.5 Val-MAE:  140.6 Median-geo:  2.901 geo 2.432 time:  38.44727563858032
1 / 250 :  Training loss:  2.5 train-MAE:  139.2 Median-Geo:  2.676 geo-train 2.492 Validation loss:  2.5 Val-MAE:  140.6 Median-geo:  2.877 geo 2.426 time:  28.998230695724487
2 / 250 :  Training loss:  2.6 train-MAE:  141.0 Median-Geo:  2.525 geo-train 2.48 Validation loss:  2.5 Val-MAE:  140.4 Median-geo:  2.829 geo 2.418 time:  27.641318798065186
3 / 250 :  Training loss:  2.6 train-MAE:  138.9 Median-Geo:  2.591 geo-train 2.385 Validation loss:  2.5 Val-MAE:  140.2 Median-geo:  2.785 geo 2.411 time:  27.769195079803467
4 / 250 :  Training loss:  2.5 train-MAE:  136.5 Median-Geo:  2.655 geo-train 2.401 Validation loss:  2.5 Val-MAE:  139.7 Median-geo:  2.738 geo 2.397 time:  27.97869348526001
5 / 250 :  Training loss:  2.5 train-MAE:  137.0 Median-Geo:  2.435 geo-train 2.362 Validation loss:  2.5 Val-MAE:  139.1 Median-geo:  2.69 geo 2.385 time:  27.702261686325073
6 / 250 :  Training loss:  2.5 train-MAE:  135.0 Median-Geo:  2.431 geo-train 2.268 Validation loss:  2.5 Val-MAE:  138.6 Median-geo:  2.637 geo 2.366 time:  27.872153759002686
7 / 250 :  Training loss:  2.5 train-MAE:  136.6 Median-Geo:  2.582 geo-train 2.38 Validation loss:  2.5 Val-MAE:  137.7 Median-geo:  2.578 geo 2.342 time:  28.861735343933105
8 / 250 :  Training loss:  2.5 train-MAE:  134.4 Median-Geo:  2.459 geo-train 2.322 Validation loss:  2.5 Val-MAE:  137.1 Median-geo:  2.543 geo 2.322 time:  29.33171272277832
9 / 250 :  Training loss:  2.5 train-MAE:  132.1 Median-Geo:  2.35 geo-train 2.185 Validation loss:  2.5 Val-MAE:  136.3 Median-geo:  2.493 geo 2.287 time:  29.22160243988037
10 / 250 :  Training loss:  2.5 train-MAE:  134.0 Median-Geo:  2.48 geo-train 2.362 Validation loss:  2.5 Val-MAE:  135.6 Median-geo:  2.455 geo 2.264 time:  29.320581197738647
11 / 250 :  Training loss:  2.5 train-MAE:  134.1 Median-Geo:  2.519 geo-train 2.379 Validation loss:  2.5 Val-MAE:  134.9 Median-geo:  2.41 geo 2.244 time:  27.984719276428223
12 / 250 :  Training loss:  2.5 train-MAE:  131.0 Median-Geo:  2.369 geo-train 2.214 Validation loss:  2.5 Val-MAE:  134.1 Median-geo:  2.373 geo 2.219 time:  27.828664779663086
13 / 250 :  Training loss:  2.5 train-MAE:  130.1 Median-Geo:  2.271 geo-train 2.257 Validation loss:  2.5 Val-MAE:  133.1 Median-geo:  2.323 geo 2.194 time:  27.804438591003418
14 / 250 :  Training loss:  2.5 train-MAE:  129.0 Median-Geo:  2.272 geo-train 2.242 Validation loss:  2.5 Val-MAE:  131.4 Median-geo:  2.22 geo 2.14 time:  28.12311625480652
15 / 250 :  Training loss:  2.4 train-MAE:  127.6 Median-Geo:  2.233 geo-train 2.214 Validation loss:  2.5 Val-MAE:  129.7 Median-geo:  2.148 geo 2.101 time:  28.297260522842407
16 / 250 :  Training loss:  2.4 train-MAE:  125.2 Median-Geo:  2.194 geo-train 2.192 Validation loss:  2.4 Val-MAE:  128.1 Median-geo:  2.105 geo 2.058 time:  29.408511638641357
17 / 250 :  Training loss:  2.4 train-MAE:  124.2 Median-Geo:  2.123 geo-train 2.119 Validation loss:  2.4 Val-MAE:  125.9 Median-geo:  2.089 geo 2.006 time:  29.236050367355347
18 / 250 :  Training loss:  2.3 train-MAE:  119.9 Median-Geo:  2.071 geo-train 2.011 Validation loss:  2.4 Val-MAE:  123.6 Median-geo:  1.997 geo 1.967 time:  27.986923694610596
19 / 250 :  Training loss:  2.4 train-MAE:  121.9 Median-Geo:  2.169 geo-train 2.201 Validation loss:  2.4 Val-MAE:  121.5 Median-geo:  1.917 geo 1.926 time:  27.915200233459473
20 / 250 :  Training loss:  2.3 train-MAE:  120.1 Median-Geo:  2.115 geo-train 2.103 Validation loss:  2.3 Val-MAE:  119.3 Median-geo:  1.828 geo 1.881 time:  27.902745246887207
21 / 250 :  Training loss:  2.3 train-MAE:  118.7 Median-Geo:  2.057 geo-train 2.064 Validation loss:  2.3 Val-MAE:  117.2 Median-geo:  1.757 geo 1.846 time:  27.867121696472168
22 / 250 :  Training loss:  2.3 train-MAE:  119.0 Median-Geo:  1.975 geo-train 2.028 Validation loss:  2.3 Val-MAE:  115.0 Median-geo:  1.662 geo 1.798 time:  27.9417622089386
23 / 250 :  Training loss:  2.2 train-MAE:  112.5 Median-Geo:  1.961 geo-train 1.977 Validation loss:  2.2 Val-MAE:  112.4 Median-geo:  1.609 geo 1.74 time:  28.29724907875061
24 / 250 :  Training loss:  2.2 train-MAE:  111.0 Median-Geo:  1.968 geo-train 1.919 Validation loss:  2.2 Val-MAE:  109.9 Median-geo:  1.55 geo 1.703 time:  29.21381187438965
25 / 250 :  Training loss:  2.1 train-MAE:  104.7 Median-Geo:  1.621 geo-train 1.776 Validation loss:  2.1 Val-MAE:  107.8 Median-geo:  1.47 geo 1.659 time:  29.17578101158142
26 / 250 :  Training loss:  2.1 train-MAE:  107.8 Median-Geo:  1.992 geo-train 1.897 Validation loss:  2.1 Val-MAE:  105.5 Median-geo:  1.404 geo 1.631 time:  28.275397062301636
27 / 250 :  Training loss:  2.1 train-MAE:  106.9 Median-Geo:  1.627 geo-train 1.733 Validation loss:  2.1 Val-MAE:  103.2 Median-geo:  1.287 geo 1.592 time:  27.881825923919678
28 / 250 :  Training loss:  2.0 train-MAE:  101.2 Median-Geo:  1.602 geo-train 1.799 Validation loss:  2.0 Val-MAE:  100.9 Median-geo:  1.242 geo 1.565 time:  28.033873081207275
29 / 250 :  Training loss:  1.9 train-MAE:  96.4 Median-Geo:  1.78 geo-train 1.752 Validation loss:  2.0 Val-MAE:  98.6 Median-geo:  1.124 geo 1.529 time:  27.727890014648438
30 / 250 :  Training loss:  1.9 train-MAE:  94.4 Median-Geo:  1.495 geo-train 1.664 Validation loss:  1.9 Val-MAE:  96.6 Median-geo:  1.09 geo 1.509 time:  27.88174343109131
31 / 250 :  Training loss:  1.9 train-MAE:  97.1 Median-Geo:  1.449 geo-train 1.59 Validation loss:  1.9 Val-MAE:  94.8 Median-geo:  1.076 geo 1.462 time:  28.01249051094055
32 / 250 :  Training loss:  1.9 train-MAE:  92.7 Median-Geo:  1.399 geo-train 1.442 Validation loss:  1.9 Val-MAE:  93.1 Median-geo:  1.096 geo 1.439 time:  27.954999685287476
33 / 250 :  Training loss:  1.8 train-MAE:  90.4 Median-Geo:  1.805 geo-train 1.641 Validation loss:  1.8 Val-MAE:  91.5 Median-geo:  1.104 geo 1.422 time:  29.11798119544983
34 / 250 :  Training loss:  1.8 train-MAE:  89.7 Median-Geo:  1.408 geo-train 1.458 Validation loss:  1.8 Val-MAE:  90.2 Median-geo:  1.118 geo 1.399 time:  27.891014337539673
35 / 250 :  Training loss:  1.8 train-MAE:  91.3 Median-Geo:  1.725 geo-train 1.672 Validation loss:  1.8 Val-MAE:  89.0 Median-geo:  1.136 geo 1.375 time:  27.791958808898926
36 / 250 :  Training loss:  1.8 train-MAE:  88.8 Median-Geo:  1.359 geo-train 1.397 Validation loss:  1.8 Val-MAE:  88.1 Median-geo:  1.139 geo 1.349 time:  27.753718852996826
37 / 250 :  Training loss:  1.8 train-MAE:  93.0 Median-Geo:  1.526 geo-train 1.612 Validation loss:  1.7 Val-MAE:  87.3 Median-geo:  1.168 geo 1.341 time:  27.847427368164062
38 / 250 :  Training loss:  1.8 train-MAE:  89.8 Median-Geo:  1.612 geo-train 1.667 Validation loss:  1.7 Val-MAE:  86.4 Median-geo:  1.173 geo 1.325 time:  28.38098907470703
39 / 250 :  Training loss:  1.7 train-MAE:  85.4 Median-Geo:  1.248 geo-train 1.459 Validation loss:  1.7 Val-MAE:  85.5 Median-geo:  1.174 geo 1.314 time:  28.636988639831543
40 / 250 :  Training loss:  1.8 train-MAE:  90.4 Median-Geo:  1.364 geo-train 1.484 Validation loss:  1.7 Val-MAE:  84.5 Median-geo:  1.189 geo 1.304 time:  29.226633310317993
41 / 250 :  Training loss:  1.7 train-MAE:  86.9 Median-Geo:  1.552 geo-train 1.587 Validation loss:  1.7 Val-MAE:  83.7 Median-geo:  1.121 geo 1.3 time:  29.418320894241333
42 / 250 :  Training loss:  1.7 train-MAE:  81.9 Median-Geo:  1.251 geo-train 1.427 Validation loss:  1.7 Val-MAE:  83.1 Median-geo:  1.165 geo 1.341 time:  27.861263513565063
43 / 250 :  Training loss:  1.7 train-MAE:  85.5 Median-Geo:  1.748 geo-train 1.588 Validation loss:  1.6 Val-MAE:  81.1 Median-geo:  1.127 geo 1.305 time:  27.866140842437744
44 / 250 :  Training loss:  1.6 train-MAE:  81.3 Median-Geo:  1.413 geo-train 1.479 Validation loss:  1.6 Val-MAE:  79.0 Median-geo:  1.111 geo 1.316 time:  28.017491340637207
45 / 250 :  Training loss:  1.6 train-MAE:  80.9 Median-Geo:  1.392 geo-train 1.497 Validation loss:  1.5 Val-MAE:  77.4 Median-geo:  1.085 geo 1.31 time:  27.84197688102722
46 / 250 :  Training loss:  1.5 train-MAE:  74.3 Median-Geo:  1.007 geo-train 1.258 Validation loss:  1.7 Val-MAE:  85.5 Median-geo:  1.209 geo 1.405 time:  27.80539584159851
47 / 250 :  Training loss:  1.6 train-MAE:  80.4 Median-Geo:  1.326 geo-train 1.362 Validation loss:  1.5 Val-MAE:  73.4 Median-geo:  1.496 geo 1.485 time:  27.89952039718628
48 / 250 :  Training loss:  1.5 train-MAE:  75.3 Median-Geo:  1.085 geo-train 1.284 Validation loss:  1.5 Val-MAE:  72.6 Median-geo:  1.347 geo 1.403 time:  28.024303674697876
49 / 250 :  Training loss:  1.6 train-MAE:  81.4 Median-Geo:  1.268 geo-train 1.49 Validation loss:  1.4 Val-MAE:  69.8 Median-geo:  1.345 geo 1.314 time:  27.82141137123108
50 / 250 :  Training loss:  1.6 train-MAE:  78.0 Median-Geo:  1.254 geo-train 1.406 Validation loss:  1.4 Val-MAE:  69.9 Median-geo:  0.947 geo 1.189 time:  27.91924810409546
51 / 250 :  Training loss:  1.4 train-MAE:  70.6 Median-Geo:  0.969 geo-train 1.212 Validation loss:  1.4 Val-MAE:  70.9 Median-geo:  1.217 geo 1.316 time:  27.946609258651733
52 / 250 :  Training loss:  1.4 train-MAE:  69.3 Median-Geo:  0.802 geo-train 1.063 Validation loss:  1.4 Val-MAE:  66.5 Median-geo:  1.149 geo 1.358 time:  27.740739583969116
53 / 250 :  Training loss:  1.5 train-MAE:  74.4 Median-Geo:  1.029 geo-train 1.267 Validation loss:  1.5 Val-MAE:  73.0 Median-geo:  1.139 geo 1.343 time:  27.870585680007935
54 / 250 :  Training loss:  1.5 train-MAE:  72.8 Median-Geo:  0.977 geo-train 1.24 Validation loss:  1.3 Val-MAE:  65.6 Median-geo:  1.192 geo 1.365 time:  27.945436000823975
55 / 250 :  Training loss:  1.3 train-MAE:  65.3 Median-Geo:  0.957 geo-train 1.201 Validation loss:  1.3 Val-MAE:  65.1 Median-geo:  1.324 geo 1.264 time:  28.500858545303345
56 / 250 :  Training loss:  1.4 train-MAE:  70.1 Median-Geo:  0.987 geo-train 1.239 Validation loss:  1.4 Val-MAE:  68.2 Median-geo:  1.24 geo 1.377 time:  27.661173343658447
57 / 250 :  Training loss:  1.5 train-MAE:  73.9 Median-Geo:  1.003 geo-train 1.222 Validation loss:  1.4 Val-MAE:  68.9 Median-geo:  1.119 geo 1.323 time:  27.745368003845215
CUDA runtime error: an illegal memory access was encountered (700) in apply_lu_batched_magma at ../aten/src/ATen/native/cuda/BatchLinearAlgebra.cpp:1920
CUDA runtime error: an illegal memory access was encountered (700) in magma_queue_destroy_internal at /opt/conda/conda-bld/magma-cuda113_1619629459349/work/interface_cuda/interface.cpp:944
CUDA runtime error: an illegal memory access was encountered (700) in magma_queue_destroy_internal at /opt/conda/conda-bld/magma-cuda113_1619629459349/work/interface_cuda/interface.cpp:945
CUDA runtime error: an illegal memory access was encountered (700) in magma_queue_destroy_internal at /opt/conda/conda-bld/magma-cuda113_1619629459349/work/interface_cuda/interface.cpp:946
python3: /opt/conda/conda-bld/magma-cuda113_1619629459349/work/interface_cuda/interface.cpp:901: void magma_queue_create_from_cuda_internal(magma_device_t, cudaStream_t, cublasHandle_t, cusparseHandle_t, magma_queue**, const char*, const char*, int): Assertion `queue->dCarray__ != __null' failed.
/var/slurm_spool/job3554780/slurm_script: line 22: 4110574 Aborted                 (core dumped) python3 main.py -c toilet -r svd
